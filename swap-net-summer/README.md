###SWAP-NET Paper
This repository contains code for the ACL 2018 paper Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks

 The code is written in Tensorflow 0.12.1 version

##SWAP-NET Parameter Settings:

-Dimension of LSTM hidden states: 200

-Dimension of word embeddings: 100

-Batch size: 16

-Vocabulary Size: 150000

-max timesteps of encoder (max input text words): 800

-max timesteps of decoder: 50

-Number of layers of LSTM: 1

-Optimizer: Adam with default parameters as,

    learning_rate=0.001,
    beta1=0.9,
    beta2=0.999
    
-Gradient Clipping Threshold: 2    

-Number of GPUs: 2,Tesla K-80

-Training time: 4 days

This code is based on [Textsum code](https://github.com/tensorflow/models/tree/master/textsum) and [pointer-generator](https://github.com/abisee/pointer-generator#help-ive-got-nans)


##Dataset
Create a new directory dataset\. Then download CNN / Daily Mail dataset from [here](https://docs.google.com/uc?id=0B0Obe9L1qtsnSXZEd0JCenIyejg&export=download).


Install RAKE from [here](https://github.com/zelandiya/RAKE-tutorial)
Install gensim from [here](https://radimrehurek.com/gensim/install.html)

##How To Run:

Now run preprosses.py

This will create following in dataset/finished_files directory:
- Files with processed data : train.bin, test.bin,val.bin 
- Vocabulary file : vocab.txt  
- File generated from RAKE : keyword.txt 
- word2vec model : embed_model.bin


#Training:

'$python training/seq2seq_attention.py'

#Evaluation:

'$python training/seq2seq_attention_eval.py'

#Extractive Summerization:
Install pyrouge to obtain rouge scores

'$python decode/seq2seq_attention_decode.py'

##Results
#Summaries Generated by Model:
Sample articles and corrosponding summaries can be found in generated_summaries/system_summary


   
Parameter Settings used by baseline Nallapati et al.:

-Dimension of LSTM hidden states: 200

-Dimension of word embeddings: 100

-Batch size: 64

-Vocabulary Size: 150000

-max timesteps of encoder (max input text words): 50 words per sentence

-Optimizer: adadelta    
